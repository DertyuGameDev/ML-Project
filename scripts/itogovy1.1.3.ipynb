{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42bde40d-a215-4e9f-b464-edaef3aba75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.v2 import ToDtype\n",
    "from torchvision.transforms import Normalize\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a94e0-ad86-4406-962b-bb0ee55ac1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразования для данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=1.5),  # Уменьшение яркости\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Загрузка категорий\n",
    "activity_categories = pd.read_csv(activity_categories_file)\n",
    "category_map = dict(zip(activity_categories['id'], activity_categories['category']))\n",
    "\n",
    "# Класс для пользовательского датасета\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.data_frame.iloc[idx, 0]) + \".jpg\")  # ID изображений в первом столбце\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.data_frame.iloc[idx, 1]  # Метки во втором столбце\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Загрузка данных\n",
    "train_dataset = CustomDataset(csv_file=csv_train_ans, root_dir=root_dir_train, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(train_dataset, val_indices)\n",
    "\n",
    "# Создание загрузчиков для обучающего и валидационного наборов\n",
    "train_loader = DataLoader(dataset=train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Определение модели (ResNet18)\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "# Инициализация модели\n",
    "num_classes = len(activity_categories)  # Количество классов в вашем датасете\n",
    "model = ResNet18(num_classes=num_classes)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # Используем CUDA или CPU\n",
    "\n",
    "print(\"Для обучения выбран девайс {}\".format(device))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "num_epochs = 120  # Увеличьте количество эпох по мере необходимости\n",
    "\n",
    "# Инициализация списков для хранения значений потерь и точности\n",
    "losses = []\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "val_losses = []  # Инициализация списка для валидационных потерь\n",
    "val_accuracies = []  # Инициализация списка для валидационных точностей\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    start_time = time.time()  # Запомнить время начала эпохи\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Обратный проход и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Сохранение меток и предсказаний для F1 Score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Обновление correct и total\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    end_time = time.time()  # Запомнить время конца эпохи\n",
    "    epoch_time = end_time - start_time  # Вычислить время эпохи\n",
    "\n",
    "    # Вычисление среднего лосса за эпоху\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Вычисление точности и F1 Score\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_all_labels = []\n",
    "    val_all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Отключаем градиенты для валидации\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            # Сохранение меток и предсказаний для F1 Score\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_all_labels.extend(val_labels.cpu().numpy())\n",
    "            val_all_preds.extend(val_predicted.cpu().numpy())\n",
    "\n",
    "            # Обновление val_correct и val_total\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    # Вычисление валидационной потери и точности\n",
    "    val_epoch_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Вывод результатов для текущей эпохи\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.4f}, Train F1 Score: {f1:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    # Сохранение чекпоинта каждые 5 эпох\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = f\"checkpoint_epoch_{epoch + 1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Чекпоинт сохранен: {checkpoint_path}\")\n",
    "\n",
    "print(\"Обучение завершено!\")\n",
    "\n",
    "\n",
    "\n",
    "# Функция для тестирования модели\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.data_frame = os.listdir(root_dir)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.data_frame[idx].replace(\".jpg\", '')  # Извлечение ID из имени файла\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, int(label)\n",
    "\n",
    "def get_result(model: torch.nn.Module, transform: transforms.Compose, root_dir_test: str, category_map: dict):\n",
    "    # Создание тестового датасета и загрузчика\n",
    "    dataset = CustomTestDataset(root_dir=root_dir_test, transform=transform)\n",
    "    dl = DataLoader(dataset, batch_size=128)\n",
    "\n",
    "    model.eval()  # Установка модели в режим оценки\n",
    "    ans = []\n",
    "\n",
    "    # Перебор данных в загрузчике\n",
    "    for img, label in tqdm(dl):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)  # Перемещение изображений на устройство\n",
    "        pred = model(img)  # Предсказания модели\n",
    "        preds = torch.argmax(pred, dim=1)\n",
    "\n",
    "        # Конкатенация меток и предсказаний\n",
    "        res = torch.cat((label.unsqueeze(1), preds.unsqueeze(1)), dim=1)\n",
    "        ans.extend(res.cpu())\n",
    "    \n",
    "    # Проверка, есть ли данные в ans\n",
    "    if not ans:\n",
    "        print(\"Ошибка: массив ans пуст. Проверьте процесс предсказания.\")\n",
    "        return\n",
    "\n",
    "    # Преобразование ans в список\n",
    "    ans = [[element.item() for element in row] for row in ans]\n",
    "\n",
    "    # Создание списка для сохранения результатов с номерами категорий\n",
    "    results_with_categories = []\n",
    "    \n",
    "    for id, pred in ans:\n",
    "        results_with_categories.append([id, pred])  # Сохраняем ID и номер предсказанной категории\n",
    "\n",
    "    # Убедитесь, что файл будет создан в текущей директории\n",
    "    output_file = 'result.csv'  # Сохраняем в текущей директории\n",
    "\n",
    "    # Запись результатов в CSV файл\n",
    "    try:\n",
    "        with open(output_file, 'w', newline=\"\") as out_file:\n",
    "            writer = csv.writer(out_file, delimiter=',')\n",
    "            writer.writerow(['id', 'target_feature'])  # Заголовки\n",
    "            writer.writerows(results_with_categories)  # Запись данных\n",
    "        print(\"Результаты успешно сохранены в\", output_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при записи в файл: {e}\")\n",
    "\n",
    "# Вызов функции для получения результатов на тестовых данных\n",
    "get_result(model, transform, root_dir_test, category_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992f1bb-a66c-4b4f-8979-a6af853cc6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразования для данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=1.5),  # Уменьшение яркости\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Загрузка категорий\n",
    "activity_categories = pd.read_csv(activity_categories_file)\n",
    "category_map = dict(zip(activity_categories['id'], activity_categories['category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da503730-6a65-4350-9196-ea55e68a3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для пользовательского датасета\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.data_frame.iloc[idx, 0]) + \".jpg\")  # ID изображений в первом столбце\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.data_frame.iloc[idx, 1]  # Метки во втором столбце\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Загрузка данных\n",
    "train_dataset = CustomDataset(csv_file=csv_train_ans, root_dir=root_dir_train, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(train_dataset, val_indices)\n",
    "\n",
    "# Создание загрузчиков для обучающего и валидационного наборов\n",
    "train_loader = DataLoader(dataset=train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_subset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae14b4-ce94-477e-a426-9aa9a84cd63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение модели (ResNet18)\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "# Инициализация модели\n",
    "num_classes = len(activity_categories)  # Количество классов в вашем датасете\n",
    "model = ResNet18(num_classes=num_classes)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # Используем CUDA или CPU\n",
    "\n",
    "print(\"Для обучения выбран девайс {}\".format(device))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221b556-05cf-4538-8cec-022194f19d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    start_time = time.time()  # Запомнить время начала эпохи\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Обратный проход и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Сохранение меток и предсказаний для F1 Score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Обновление correct и total\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    end_time = time.time()  # Запомнить время конца эпохи\n",
    "    epoch_time = end_time - start_time  # Вычислить время эпохи\n",
    "\n",
    "    # Вычисление среднего лосса за эпоху\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Вычисление точности и F1 Score\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_all_labels = []\n",
    "    val_all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Отключаем градиенты для валидации\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            # Сохранение меток и предсказаний для F1 Score\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_all_labels.extend(val_labels.cpu().numpy())\n",
    "            val_all_preds.extend(val_predicted.cpu().numpy())\n",
    "\n",
    "            # Обновление val_correct и val_total\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    # Вычисление валидационной потери и точности\n",
    "    val_epoch_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Вывод результатов для текущей эпохи\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.4f}, Train F1 Score: {f1:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    # Сохранение чекпоинта каждые 5 эпох\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = f\"checkpoint_epoch_{epoch + 1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Чекпоинт сохранен: {checkpoint_path}\")\n",
    "\n",
    "print(\"Обучение завершено!\")\n",
    "\n",
    "\n",
    "\n",
    "# Функция для тестирования модели\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.data_frame = os.listdir(root_dir)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.data_frame[idx].replace(\".jpg\", '')  # Извлечение ID из имени файла\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, int(label)\n",
    "\n",
    "def get_result(model: torch.nn.Module, transform: transforms.Compose, root_dir_test: str, category_map: dict):\n",
    "    # Создание тестового датасета и загрузчика\n",
    "    dataset = CustomTestDataset(root_dir=root_dir_test, transform=transform)\n",
    "    dl = DataLoader(dataset, batch_size=128)\n",
    "\n",
    "    model.eval()  # Установка модели в режим оценки\n",
    "    ans = []\n",
    "\n",
    "    # Перебор данных в загрузчике\n",
    "    for img, label in tqdm(dl):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)  # Перемещение изображений на устройство\n",
    "        pred = model(img)  # Предсказания модели\n",
    "        preds = torch.argmax(pred, dim=1)\n",
    "\n",
    "        # Конкатенация меток и предсказаний\n",
    "        res = torch.cat((label.unsqueeze(1), preds.unsqueeze(1)), dim=1)\n",
    "        ans.extend(res.cpu())\n",
    "    \n",
    "    # Проверка, есть ли данные в ans\n",
    "    if not ans:\n",
    "        print(\"Ошибка: массив ans пуст. Проверьте процесс предсказания.\")\n",
    "        return\n",
    "\n",
    "    # Преобразование ans в список\n",
    "    ans = [[element.item() for element in row] for row in ans]\n",
    "\n",
    "    # Создание списка для сохранения результатов с номерами категорий\n",
    "    results_with_categories = []\n",
    "    \n",
    "    for id, pred in ans:\n",
    "        results_with_categories.append([id, pred])  # Сохраняем ID и номер предсказанной категории\n",
    "\n",
    "    # Убедитесь, что файл будет создан в текущей директории\n",
    "    output_file = 'result.csv'  # Сохраняем в текущей директории\n",
    "\n",
    "    # Запись результатов в CSV файл\n",
    "    try:\n",
    "        with open(output_file, 'w', newline=\"\") as out_file:\n",
    "            writer = csv.writer(out_file, delimiter=',')\n",
    "            writer.writerow(['id', 'target_feature'])  # Заголовки\n",
    "            writer.writerows(results_with_categories)  # Запись данных\n",
    "        print(\"Результаты успешно сохранены в\", output_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при записи в файл: {e}\")\n",
    "\n",
    "# Вызов функции для получения результатов на тестовых данных\n",
    "get_result(model, transform, root_dir_test, category_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c8257-1254-492f-826c-c3d83af671f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
