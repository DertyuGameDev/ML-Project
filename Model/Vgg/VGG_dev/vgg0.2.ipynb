{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9b8b3b-8096-4abd-b563-1f6adde35043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b039ef6b-7620-4d10-8b38-074ef0d98719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms.v2 import ToDtype\n",
    "from torchvision.transforms import Normalize\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomRotation, RandomVerticalFlip, ColorJitter, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f836f3-c2ac-46bc-94d7-d29b5ee8bec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]             896\n",
      "              ReLU-2         [-1, 32, 224, 224]               0\n",
      "       BatchNorm2d-3         [-1, 32, 224, 224]              64\n",
      "            Conv2d-4         [-1, 32, 224, 224]           9,248\n",
      "              ReLU-5         [-1, 32, 224, 224]               0\n",
      "       BatchNorm2d-6         [-1, 32, 224, 224]              64\n",
      "         MaxPool2d-7         [-1, 32, 112, 112]               0\n",
      "            Conv2d-8         [-1, 64, 112, 112]          18,496\n",
      "              ReLU-9         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-10         [-1, 64, 112, 112]             128\n",
      "           Conv2d-11         [-1, 64, 112, 112]          36,928\n",
      "             ReLU-12         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-13         [-1, 64, 112, 112]             128\n",
      "        MaxPool2d-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15          [-1, 128, 56, 56]          73,856\n",
      "             ReLU-16          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-17          [-1, 128, 56, 56]             256\n",
      "           Conv2d-18          [-1, 128, 56, 56]         147,584\n",
      "             ReLU-19          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
      "        MaxPool2d-21          [-1, 128, 28, 28]               0\n",
      "           Linear-22                  [-1, 512]      51,380,736\n",
      "             ReLU-23                  [-1, 512]               0\n",
      "          Dropout-24                  [-1, 512]               0\n",
      "           Linear-25                  [-1, 256]         131,328\n",
      "             ReLU-26                  [-1, 256]               0\n",
      "          Dropout-27                  [-1, 256]               0\n",
      "           Linear-28                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 51,802,538\n",
      "Trainable params: 51,802,538\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 134.00\n",
      "Params size (MB): 197.61\n",
      "Estimated Total Size (MB): 332.19\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10:   0%|                                                                                                                              | 0/387 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Kirill\\\\PycharmProjects\\\\nto\\\\img_train\\\\6652'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 129\u001b[0m\n\u001b[0;32m    125\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    127\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 129\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Epoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 31\u001b[0m, in \u001b[0;36mCustomTrainDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m img_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Преобразуем в строку\u001b[39;00m\n\u001b[0;32m     30\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, img_name)\n\u001b[1;32m---> 31\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Убедитесь, что метка - это целое число\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\PIL\\Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Kirill\\\\PycharmProjects\\\\nto\\\\img_train\\\\6652'"
     ]
    }
   ],
   "source": [
    "# Устройство для вычислений\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Пути к данным\n",
    "source_directory = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data'\n",
    "csv_train_ans = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\train_answers.csv'\n",
    "root_dir_train = r'img_train'\n",
    "root_dir_test = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_test'\n",
    "activity_categories_file = r'activity_categories.csv'\n",
    "\n",
    "# Гиперпараметры\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_classes = 10  # Укажите количество классов в задаче\n",
    "\n",
    "\n",
    "# Класс для загрузки тестового датасета\n",
    "class CustomTrainDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotations_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Преобразование значения в строку\n",
    "        img_name = str(self.annotations.iloc[idx, 0])  # Преобразуем в строку\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = int(self.annotations.iloc[idx, 1])  # Убедитесь, что метка - это целое число\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Определение трансформаций\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Загрузка данных\n",
    "train_dataset = CustomTrainDataset(annotations_file=csv_train_ans, root_dir=root_dir_train, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Создание валидационного набора данных (например, 20% от тренировочного)\n",
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Модель VGG\n",
    "class ImprovedVGG(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ImprovedVGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 28 * 28, 512),  # Adjusted for the output size after feature extraction\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, num_classes)  # Output layer for the number of classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Инициализация модели\n",
    "model = ImprovedVGG(num_classes=num_classes).to(device)\n",
    "summary(model, (3, 224, 224))  # Измените размер входа на (3, 224, 224)\n",
    "\n",
    "# Оптимизатор и функция потерь\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Планировщик\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Training Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_all_labels = []\n",
    "    val_all_preds = []\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            val_images = val_images.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_all_labels.extend(val_labels.cpu().numpy())\n",
    "            val_all_preds.extend(val_predicted.cpu().numpy())\n",
    "\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_f1 = f1_score(val_all_labels, val_all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Validation Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Loss: {val_epoch_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "    # Обновление планировщика\n",
    "    scheduler.step()\n",
    "\n",
    "    # Сохранение чекпоинта каждые 5 эпох\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = f\"checkpoint_epoch_{epoch + 1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Функция для получения результатов\n",
    "def get_result(model: torch.nn.Module, transform: transforms.Compose, root_dir_test: str):\n",
    "    dataset = CustomTestDataset(root_dir=root_dir_test, transform=transform)\n",
    "    dl = DataLoader(dataset, batch_size=128)\n",
    "\n",
    "    model.eval()\n",
    "    ans = []\n",
    "\n",
    "    for img, label in tqdm(dl):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        pred = model(img)\n",
    "        preds = torch.argmax(pred, dim=1)\n",
    "\n",
    "        res = torch.cat((label.unsqueeze(1), preds.unsqueeze(1)), dim=1)\n",
    "        ans.extend(res.cpu())\n",
    "\n",
    "    if not ans:\n",
    "        print(\"Error: ans array is empty. Check the prediction process.\")\n",
    "        return\n",
    "\n",
    "    ans = [[element.item() for element in row] for row in ans]\n",
    "\n",
    "    output_file = 'result.csv'\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'w', newline=\"\") as out_file:\n",
    "            writer = csv.writer(out_file, delimiter=',')\n",
    "            writer.writerow(['id', 'target_feature'])\n",
    "            writer.writerows(ans)\n",
    "        print(\"Results saved to\", output_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "\n",
    "# Вызов функции для тестирования\n",
    "get_result(model, transform, root_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e965e6e-7501-41f0-8e31-be33ef284ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirill\\AppData\\Local\\Temp\\1\\ipykernel_23856\\2245471223.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0        17363\n",
      "1         8612\n",
      "2         8244\n",
      "3         9264\n",
      "4         5382\n",
      "         ...  \n",
      "12362    11503\n",
      "12363    12201\n",
      "12364     5489\n",
      "12365      883\n",
      "12366    16116\n",
      "Name: img_id, Length: 12367, dtype: object' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_annotations.iloc[:, 0] = train_annotations.iloc[:, 0].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  img_id  target_feature\n",
      "0  17363               2\n",
      "1   8612               5\n",
      "2   8244               0\n",
      "3   9264               0\n",
      "4   5382               6\n",
      "  img_id  target_feature\n",
      "0  17363               2\n",
      "1   8612               5\n",
      "2   8244               0\n",
      "3   9264               0\n",
      "4   5382               6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10:   0%|                                                                                                                              | 0/387 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\824\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\15030\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\14681\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\8017\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\16034\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\12081\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\10236\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\98\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\7523\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\12040\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\4810\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\9700\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\9871\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\3208\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\9259\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\17280\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\17488\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\9882\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\2551\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\1\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\189\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\16634\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\9082\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\14199\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\6458\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\17761\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\9064\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\17072\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\6836\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\11234\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\11260\n",
      "Файл не найден: D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train\\580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     90\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 92\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Epoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mcontinue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Пропускаем итерацию, если изображение или метка отсутствуют\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    238\u001b[0m             ]\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "\n",
    "# Установка устройства\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Пути к данным\n",
    "source_directory = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data'\n",
    "csv_train_ans = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\train_answers.csv'\n",
    "root_dir_train = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train'\n",
    "root_dir_test = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_test'\n",
    "activity_categories_file = r'activity_categories.csv'\n",
    "\n",
    "# Загрузка категорий активности\n",
    "activity_categories = pd.read_csv(activity_categories_file)\n",
    "activity_labels = activity_categories['category'].tolist()\n",
    "\n",
    "# Загрузка данных\n",
    "train_annotations = pd.read_csv(csv_train_ans)\n",
    "\n",
    "# Преобразуем первый столбец в строки, если это необходимо\n",
    "train_annotations.iloc[:, 0] = train_annotations.iloc[:, 0].astype(str)\n",
    "\n",
    "# Проверяем структуру данных\n",
    "print(train_annotations.head())\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Файл не найден: {img_path}\")\n",
    "            return None, None  # Или выбросьте исключение\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.annotations.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Загрузка данных\n",
    "train_annotations = pd.read_csv(csv_train_ans)\n",
    "\n",
    "# Приведение типов\n",
    "train_annotations['img_id'] = train_annotations['img_id'].astype(str)\n",
    "\n",
    "# Проверяем структуру данных\n",
    "print(train_annotations.head())\n",
    "\n",
    "# Загрузка обучающего набора данных\n",
    "train_dataset = CustomDataset(root_dir_train, train_annotations, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Трансформации\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Загрузка обучающего и тестового наборов данных\n",
    "train_dataset = CustomDataset(root_dir_train, train_annotations, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Создание модели VGG\n",
    "model = models.vgg16(weights='DEFAULT')  # Используем 'DEFAULT' для загрузки предобученных весов\n",
    "num_classes = len(activity_labels)\n",
    "\n",
    "# Изменяем последний слой\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Обучение модели\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        if images is None or labels is None:\n",
    "            continue  # Пропускаем итерацию, если изображение или метка отсутствуют\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Обнуление градиентов\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Обратный проход и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Обновление потерь\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        total += labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Вычисление среднего лосса за эпоху\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Сохранение модели\n",
    "torch.save(model.state_dict(), 'vgg16_finetuned.pth')\n",
    "print(\"Модель сохранена как 'vgg16_finetuned.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d247e4f7-b80f-4132-b28f-d6104f979961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'category'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "activity_categories = pd.read_csv(activity_categories_file)\n",
    "print(activity_categories.columns)  # Выводим имена столбцов\n",
    "activity_labels = activity_categories['category'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58499370-b49d-4b49-b3f5-67bed1f737f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"Файл не найден: {img_path}\")  # Выводим сообщение, если файл не найден\n",
    "        # Вы можете вернуть пустое изображение или выбросить исключение\n",
    "        return None, None\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    label = self.annotations.iloc[idx, 1]\n",
    "\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75ac6e73-2a02-4f26-80a4-1b4774fb8fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  img_id  target_feature\n",
      "0  17363               2\n",
      "1   8612               5\n",
      "2   8244               0\n",
      "3   9264               0\n",
      "4   5382               6\n"
     ]
    }
   ],
   "source": [
    "print(train_annotations.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "def66d9f-7661-45d8-bfe9-1acdf375ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Файл не найден: {img_path}\")\n",
    "            return None, None  # Или выбросьте исключение\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.annotations.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7dbb765-62ba-430f-bc39-9a616a0e345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для обучения выбран девайс cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # Используем CUDA или CPU\n",
    "\n",
    "print(\"Для обучения выбран девайс {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b65953ce-8709-477d-abea-ebc07e2a2e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "csv_train_ans = pd.read_csv('train_answers.csv')\n",
    "\n",
    "# Проверка типа переменной\n",
    "print(type(csv_train_ans))  # Должно быть <class 'pandas.core.frame.DataFrame'>\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Итерация по строкам DataFrame\n",
    "for index, row in csv_train_ans.iterrows():\n",
    "    image_id = row['img_id']  # Используйте имя столбца\n",
    "    action = row['target_feature']  # Используйте имя столбца\n",
    "    \n",
    "    image_path = os.path.join('D:\\\\data sets\\\\ml-intensive-yandex-academy-autumn-2024\\\\human_poses_data', 'D:\\\\data sets\\\\ml-intensive-yandex-academy-autumn-2024\\\\human_poses_data\\\\img_train', f'{image_id}.jpg')\n",
    "    X.append(image_path)\n",
    "    y.append(action)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=1.5),  # Уменьшение яркости\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Применение трансформаций к первому изображению\n",
    "image = Image.open(X[0])  # Открываем изображение\n",
    "transformed_image = transform(image)  # Применяем трансформации\n",
    "print(transformed_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7474945c-39f8-4636-a5b5-2014b9b3497b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   4%|████▉                                                                                                                           | 3/78 [00:31<12:57, 10.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 141\u001b[0m\n\u001b[0;32m    138\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    139\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 141\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Сохранение меток и предсказаний для F1 Score\u001b[39;00m\n\u001b[0;32m    144\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "source_directory = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data'\n",
    "csv_train_ans = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\train_answers.csv'\n",
    "root_dir_train = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train'\n",
    "root_dir_test = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_test'\n",
    "activity_categories_file = r'activity_categories.csv'\n",
    "\n",
    "# Преобразования для данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=1.5),  # Уменьшение яркости\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "initial_trans = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Загрузка категорий\n",
    "activity_categories = pd.read_csv(activity_categories_file)\n",
    "category_map = dict(zip(activity_categories['id'], activity_categories['category']))\n",
    "\n",
    "# Класс для пользовательского датасета\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.data_frame.iloc[idx, 0]) + \".jpg\")  # ID изображений в первом столбце\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.data_frame.iloc[idx, 1]  # Метки во втором столбце\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Загрузка данных\n",
    "train_dataset = CustomDataset(csv_file=csv_train_ans, root_dir=root_dir_train, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(train_dataset, val_indices)\n",
    "\n",
    "# Создание загрузчиков для обучающего и валидационного наборов\n",
    "train_loader = DataLoader(dataset=train_subset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "class ImprovedVGG(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(ImprovedVGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Измените размер в соответствии с выходом после сверток\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 28 * 28, 512),  # Убедитесь, что размер соответствует выходу\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, num_classes)  # Выходной слой для количества классов\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Преобразование тензора в одномерный\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Инициализация модели\n",
    "num_classes = len(activity_categories)  # Количество классов в вашем датасете\n",
    "model = ImprovedVGG(num_classes=num_classes).to(device)   # Количество классов в вашем датасете\n",
    "# model = ResNet18(num_classes=num_classes)\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# Обучение модели\n",
    "num_epochs = 20  # Установите количество эпох\n",
    "losses = []\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    start_time = time.time()  # Запомнить время начала эпохи\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)  # Перемещение данных на устройство\n",
    "\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Обратный проход и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Сохранение меток и предсказаний для F1 Score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Обновление correct и total\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    end_time = time.time()  # Запомнить время конца эпохи\n",
    "    epoch_time = end_time - start_time  # Вычислить время эпохи\n",
    "\n",
    "    # Вычисление среднего лосса за эпоху\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Вычисление точности и F1 Score\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_all_labels = []\n",
    "    val_all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Отключаем градиенты для валидации\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            # Сохранение меток и предсказаний для F1 Score\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_all_labels.extend(val_labels.cpu().numpy())\n",
    "            val_all_preds.extend(val_predicted.cpu().numpy())\n",
    "\n",
    "            # Обновление val_correct и val_total\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    # Вычисление валидационной потери и точности\n",
    "    val_epoch_loss = val_running_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    # Вывод результатов для текущей эпохи\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.4f}, Train F1 Score: {f1:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Сохранение чекпоинта каждые 5 эпох\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = f\"checkpoint_epoch_{epoch + 1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Чекпоинт сохранен: {checkpoint_path}\")\n",
    "\n",
    "print(\"Обучение завершено!\")\n",
    "\n",
    "\n",
    "# Функция для тестирования модели\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.data_frame = os.listdir(root_dir)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.data_frame[idx].replace(\".jpg\", '')  # Извлечение ID из имени файла\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, int(label)\n",
    "\n",
    "def get_result(model: torch.nn.Module, transform: transforms.Compose, root_dir_test: str, category_map: dict):\n",
    "    # Создание тестового датасета и загрузчика\n",
    "    dataset = CustomTestDataset(root_dir=root_dir_test, transform=transform)\n",
    "    dl = DataLoader(dataset, batch_size=128)\n",
    "\n",
    "    model.eval()  # Установка модели в режим оценки\n",
    "    ans = []\n",
    "\n",
    "    # Перебор данных в загрузчике\n",
    "    for img, label in tqdm(dl):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)  # Перемещение изображений на устройство\n",
    "        pred = model(img)  # Предсказания модели\n",
    "        preds = torch.argmax(pred, dim=1)\n",
    "\n",
    "        # Конкатенация меток и предсказаний\n",
    "        res = torch.cat((label.unsqueeze(1), preds.unsqueeze(1)), dim=1)\n",
    "        ans.extend(res.cpu())\n",
    "    \n",
    "    # Проверка, есть ли данные в ans\n",
    "    if not ans:\n",
    "        print(\"Ошибка: массив ans пуст. Проверьте процесс предсказания.\")\n",
    "        return\n",
    "\n",
    "    # Преобразование ans в список\n",
    "    ans = [[element.item() for element in row] for row in ans]\n",
    "\n",
    "    # Создание списка для сохранения результатов с номерами категорий\n",
    "    results_with_categories = []\n",
    "    \n",
    "    for id, pred in ans:\n",
    "        results_with_categories.append([id, pred])  # Сохраняем ID и номер предсказанной категории\n",
    "\n",
    "    # Убедитесь, что файл будет создан в текущей директории\n",
    "    output_file = 'result.csv'  # Сохраняем в текущей директории\n",
    "\n",
    "    # Запись результатов в CSV файл\n",
    "    try:\n",
    "        with open(output_file, 'w', newline=\"\") as out_file:\n",
    "            writer = csv.writer(out_file, delimiter=',')\n",
    "            writer.writerow(['id', 'target_feature'])  # Заголовки\n",
    "            writer.writerows(results_with_categories)  # Запись данных\n",
    "        print(\"Результаты успешно сохранены в\", output_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при записи в файл: {e}\")\n",
    "\n",
    "# Вызов функции для получения результатов на тестовых данных\n",
    "get_result(model, transform, root_dir_test, category_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521d4a1-6d91-4e07-87ad-c5620e6f7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "initial_trans = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert the tensor back to a PIL image\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.RandomAffine(degrees=(0, 20), translate=(0.1, 0.1)),\n",
    "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "    transforms.ToTensor(),  # Convert back to tensor after augmentations\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))  # Normalize the tensor\n",
    "])\n",
    "\n",
    "train_answers = pd.read_csv('human_poses_data/train_answers.csv', delimiter=';')\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for index, row in train_answers.iterrows():\n",
    "    image_id = row[0]\n",
    "    action = row[1]\n",
    "    \n",
    "    image_path = os.path.join('human_poses_data', 'img_train', f'{image_id}.jpg')\n",
    "    X.append(image_path)\n",
    "    y.append(action)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def init(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def getitem(self, idx):\n",
    "        return initial_trans(Image.open(self.images[idx])), self.labels[idx]\n",
    "\n",
    "dataset = CustomDataset(X, y)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_set, valid_set = random_split(dataset, (0.95, 0.05))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=24)\n",
    "valid_loader = DataLoader(valid_set, batch_size=24)\n",
    "train_loader\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, num_epochs): \n",
    "    train_losses, val_losses = [], [] \n",
    "    val_metrics = [] \n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "        model.train() \n",
    "        train_loss = 0 \n",
    "        for images, labels in train_loader: \n",
    "            im =  []\n",
    "            for i in images:\n",
    "                im.append(transforms(i))\n",
    "            images, labels = torch.stack(im), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images) \n",
    "            loss = criterion(outputs, labels) \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            train_loss += loss.item() \n",
    "\n",
    "        train_loss /= len(train_loader) \n",
    "        train_losses.append(train_loss) \n",
    "        \n",
    "        # Validation\n",
    "        model.eval() \n",
    "        val_loss = 0\n",
    "        all_preds = [] \n",
    "        all_labels = [] \n",
    "        with torch.no_grad(): \n",
    "            for images, labels in val_loader: \n",
    "                images, labels = images.to(device), labels.to(device) \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels) \n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1) \n",
    "                all_preds.extend(preds.cpu().numpy()) \n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss) \n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        acc = accuracy_score(all_labels, all_preds) \n",
    "        prec = precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0) \n",
    "        rec = recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0) \n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0) \n",
    "        val_metrics.append((acc, prec, rec, f1)) \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Acc: {acc:.4f}, Val Prec: {prec:.4f}, Val Recall: {rec:.4f}, Val F1: {f1:.4f}\") \n",
    "\n",
    "train_and_evaluate(model, train_loader, valid_loader, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62723786-e23e-4ab2-a75c-71b01a91f4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для обучения выбран девайс cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # Используем CUDA или CPU\n",
    "\n",
    "print(\"Для обучения выбран девайс {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f060c7-3704-4cbb-ba0a-8f846302c054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/44: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [17:58<00:00, 22.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/44], Train Loss: 4.3650, Train Accuracy: 0.1529, Train F1 Score: 0.1479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/44: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [15:10<00:00, 18.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/44], Train Loss: 2.5993, Train Accuracy: 0.2114, Train F1 Score: 0.1798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/44:  41%|███████████████████████████████████████████████████▊                                                                           | 20/49 [04:55<07:08, 14.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 155\u001b[0m\n\u001b[0;32m    151\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    153\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Запомнить время начала эпохи\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Перемещение данных на устройство\u001b[39;49;00m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Обнуляем градиенты\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 68\u001b[0m, in \u001b[0;36mValidationDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     67\u001b[0m     img_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_frame\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# ID изображений в первом столбце\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_frame\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Метки во втором столбце\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\PIL\\Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 995\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\nto\\.venv\\Lib\\site-packages\\PIL\\ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Параметры\n",
    "source_directory = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data'\n",
    "csv_train_ans = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\train_answers.csv'\n",
    "root_dir_train = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_train'\n",
    "root_dir_test = r'D:\\data sets\\ml-intensive-yandex-academy-autumn-2024\\human_poses_data\\img_test'\n",
    "activity_categories_file = r'activity_categories.csv'\n",
    "\n",
    "# Преобразования для данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224, 224),\n",
    "    transforms.RandomRotation(3),\n",
    "    transforms.ColorJitter(brightness=1.5, contrast=1, saturation=0.2, hue=1), \n",
    "    transforms.RandomApply([transforms.RandomAffine(degrees=2, translate=(0.1, 0.1))]),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=1, sigma=(0.1, 2)), \n",
    "    transforms.RandomGrayscale()]), transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomChoice([transforms.RandomAdjustSharpness(1), \n",
    "    transforms.RandAugment()]), transforms.ToTensor(),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "initial_trans = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Загрузка категорий\n",
    "activity_categories = pd.read_csv(activity_categories_file)\n",
    "category_map = dict(zip(activity_categories['id'], activity_categories['category']))\n",
    "\n",
    "# Класс для пользовательского датасета\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.data_frame.iloc[idx, 0]) + \".jpg\")  # ID изображений в первом столбце\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.data_frame.iloc[idx, 1]  # Метки во втором столбце\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Загрузка данных\n",
    "train_dataset = CustomDataset(csv_file=csv_train_ans, root_dir=root_dir_train, transform=transform)\n",
    "train_indices, val_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(train_dataset, val_indices)\n",
    "\n",
    "# Создание загрузчиков для обучающего и валидационного наборов\n",
    "train_loader = DataLoader(dataset=train_subset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_subset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Класс для валидационного датасета с использованием initial_trans\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.data_frame.iloc[idx, 0]) + \".jpg\")  # ID изображений в первом столбце\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.data_frame.iloc[idx, 1]  # Метки во втором столбце\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Создание валидационного загрузчика с использованием initial_trans\n",
    "val_dataset = ValidationDataset(csv_file=csv_train_ans, root_dir=root_dir_train, transform=initial_trans)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Определение модели, функции потерь и оптимизатора\n",
    "class ImprovedVGG(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(ImprovedVGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Измените размер в соответствии с выходом после сверток\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 28 * 28, 512),  # Убедитесь, что размер соответствует выходу\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, num_classes)  # Выходной слой для количества классов\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Преобразование тензора в одномерный\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Инициализация модели\n",
    "num_classes = len(activity_categories)  # Количество классов в вашем датасете\n",
    "model = ImprovedVGG(num_classes=num_classes).to(device)  # Перемещение модели на устройство\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# Обучение модели\n",
    "num_epochs = 44  # Установите количество эпох\n",
    "losses = []\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    start_time = time.time()  # Запомнить время начала эпохи\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Обратный проход и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Сохранение меток и предсказаний для F1 Score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Обновление correct и total\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    end_time = time.time()  # Запомнить время конца эпохи\n",
    "    epoch_time = end_time - start_time  # Вычислить время эпохи\n",
    "\n",
    "    # Вычисление среднего лосса за эпоху\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Вычисление точности и F1 Score\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_all_labels = []\n",
    "    val_all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Отключаем градиенты для валидации\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            # Сохранение меток и предсказаний для F1 Score\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_all_labels.extend(val_labels.cpu().numpy())\n",
    "            val_all_preds.extend(val_predicted.cpu().numpy())\n",
    "\n",
    "            # Обновление val_correct и val_total\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    # Вычисление валидационной потери и точности\n",
    "    val_epoch_loss = val_running_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    # Вывод результатов для текущей эпохи\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.4f}, Train F1 Score: {f1:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Сохранение чекпоинта каждые 5 эпох\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = f\"checkpoint_epoch_{epoch + 1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Чекпоинт сохранен: {checkpoint_path}\")\n",
    "\n",
    "print(\"Обучение завершено!\")\n",
    "\n",
    "# Функция для тестирования модели\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.data_frame = os.listdir(root_dir)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.data_frame[idx].replace(\".jpg\", '')  # Извлечение ID из имени файла\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, int(label)\n",
    "\n",
    "def get_result(model: torch.nn.Module, transform: transforms.Compose, root_dir_test: str, category_map: dict):\n",
    "    # Создание тестового датасета и загрузчика\n",
    "    dataset = CustomTestDataset(root_dir=root_dir_test, transform=transform)\n",
    "    dl = DataLoader(dataset, batch_size=128)\n",
    "\n",
    "    model.eval()  # Установка модели в режим оценки\n",
    "    ans = []\n",
    "\n",
    "    # Перебор данных в загрузчике\n",
    "    for img, label in tqdm(dl):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)  # Перемещение изображений на устройство\n",
    "        pred = model(img)  # Предсказания модели\n",
    "        preds = torch.argmax(pred, dim=1)\n",
    "\n",
    "        # Конкатенация меток и предсказаний\n",
    "        res = torch.cat((label.unsqueeze(1), preds.unsqueeze(1)), dim=1)\n",
    "        ans.extend(res.cpu())\n",
    "    \n",
    "    # Проверка, есть ли данные в ans\n",
    "    if not ans:\n",
    "        print(\"Ошибка: массив ans пуст. Проверьте процесс предсказания.\")\n",
    "        return\n",
    "\n",
    "    # Преобразование ans в список\n",
    "    ans = [[element.item() for element in row] for row in ans]\n",
    "\n",
    "    # Создание списка для сохранения результатов с номерами категорий\n",
    "    results_with_categories = []\n",
    "    \n",
    "    for id, pred in ans:\n",
    "        results_with_categories.append([id, pred])  # Сохраняем ID и номер предсказанной категории\n",
    "\n",
    "    # Убедитесь, что файл будет создан в текущей директории\n",
    "    output_file = 'result.csv'  # Сохраняем в текущей директории\n",
    "\n",
    "    # Запись результатов в CSV файл\n",
    "    try:\n",
    "        with open(output_file, 'w', newline=\"\") as out_file:\n",
    "            writer = csv.writer(out_file, delimiter=',')\n",
    "            writer.writerow(['id', 'target_feature'])  # Заголовки\n",
    "            writer.writerows(results_with_categories)  # Запись данных\n",
    "        print(\"Результаты успешно сохранены в\", output_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при записи в файл: {e}\")\n",
    "\n",
    "# Вызов функции для получения результатов на тестовых данных\n",
    "get_result(model, transform, root_dir_test, category_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4463f-4307-4a23-aa53-76a438119fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50001c8-b4dd-4df0-bdb3-a1361c4d4266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
